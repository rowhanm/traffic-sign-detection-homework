{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import PIL.Image as Image\n",
    "\n",
    "import torchvision\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import initialize_data, training_transforms, test_transforms # data.py in the same folder\n",
    "initialize_data('images/') # extracts the zip files, makes a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.Resize((30, 30)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.3337, 0.3064, 0.3171), ( 0.2672, 0.2564, 0.2629))\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((30, 30)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.3337, 0.3064, 0.3171), ( 0.2672, 0.2564, 0.2629))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder('images/train_images',\n",
    "                         transform=training_transforms),\n",
    "    batch_size=512, shuffle=True, num_workers = 4)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder('images/val_images',\n",
    "                         transform=test_transforms),\n",
    "    batch_size=512, shuffle=False, num_workers =4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclasses = 43 # GTSRB as 43 classes\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(500, 50)\n",
    "        self.fc2 = nn.Linear(50, nclasses)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 500)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import ResNet, BasicBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTSRB_ResNet(ResNet):\n",
    "    def __init__(self):\n",
    "        super(GTSRB_ResNet, self).__init__(BasicBlock, [2, 2, 2, 2], num_classes=nclasses)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(\n",
    "            super(GTSRB_ResNet, self).forward(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=X_train.shape[1:]))\n",
    "# model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "# model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(rate=0.25))\n",
    "# model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "# model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(rate=0.25))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dropout(rate=0.5))\n",
    "# model.add(Dense(43, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv4Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv4Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(64*13*13, 256)\n",
    "        self.fc2 = nn.Linear(256, nclasses)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.dropout(x, training=self.training, p=0.25)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.dropout(x, training=self.training, p=0.25)\n",
    "        x = x.view(-1, 64*13*13)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training, p=0.5)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv4Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).to(device), Variable(target).to(device)\n",
    "        model.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation():\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in val_loader:\n",
    "        data, target = Variable(data, volatile=True).to(device), Variable(target).to(device)\n",
    "        output = model(data)\n",
    "        validation_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    validation_loss /= len(val_loader.dataset)\n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        validation_loss, correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'conv4_25_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rm5310/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/35339 (0%)]\tLoss: 3.765448\n",
      "Train Epoch: 1 [5120/35339 (14%)]\tLoss: 3.315608\n",
      "Train Epoch: 1 [10240/35339 (29%)]\tLoss: 2.815134\n",
      "Train Epoch: 1 [15360/35339 (43%)]\tLoss: 2.210364\n",
      "Train Epoch: 1 [20480/35339 (57%)]\tLoss: 1.980068\n",
      "Train Epoch: 1 [25600/35339 (71%)]\tLoss: 1.737659\n",
      "Train Epoch: 1 [30720/35339 (86%)]\tLoss: 1.528002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rm5310/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n",
      "/home/rm5310/pyenv/py3.6.3/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation set: Average loss: 2.1321, Accuracy: 1543/3870 (39%)\n",
      "\n",
      "Train Epoch: 2 [0/35339 (0%)]\tLoss: 1.354611\n",
      "Train Epoch: 2 [5120/35339 (14%)]\tLoss: 1.263055\n",
      "Train Epoch: 2 [10240/35339 (29%)]\tLoss: 1.019862\n",
      "Train Epoch: 2 [15360/35339 (43%)]\tLoss: 1.021282\n",
      "Train Epoch: 2 [20480/35339 (57%)]\tLoss: 0.823481\n",
      "Train Epoch: 2 [25600/35339 (71%)]\tLoss: 0.793937\n",
      "Train Epoch: 2 [30720/35339 (86%)]\tLoss: 0.680789\n",
      "\n",
      "Validation set: Average loss: 1.2110, Accuracy: 2512/3870 (64%)\n",
      "\n",
      "Train Epoch: 3 [0/35339 (0%)]\tLoss: 0.667673\n",
      "Train Epoch: 3 [5120/35339 (14%)]\tLoss: 0.594134\n",
      "Train Epoch: 3 [10240/35339 (29%)]\tLoss: 0.537711\n",
      "Train Epoch: 3 [15360/35339 (43%)]\tLoss: 0.590268\n",
      "Train Epoch: 3 [20480/35339 (57%)]\tLoss: 0.420641\n",
      "Train Epoch: 3 [25600/35339 (71%)]\tLoss: 0.472171\n",
      "Train Epoch: 3 [30720/35339 (86%)]\tLoss: 0.407196\n",
      "\n",
      "Validation set: Average loss: 0.9598, Accuracy: 2849/3870 (73%)\n",
      "\n",
      "Train Epoch: 4 [0/35339 (0%)]\tLoss: 0.509261\n",
      "Train Epoch: 4 [5120/35339 (14%)]\tLoss: 0.528351\n",
      "Train Epoch: 4 [10240/35339 (29%)]\tLoss: 0.428394\n",
      "Train Epoch: 4 [15360/35339 (43%)]\tLoss: 0.466886\n",
      "Train Epoch: 4 [20480/35339 (57%)]\tLoss: 0.385681\n",
      "Train Epoch: 4 [25600/35339 (71%)]\tLoss: 0.344272\n",
      "Train Epoch: 4 [30720/35339 (86%)]\tLoss: 0.355941\n",
      "\n",
      "Validation set: Average loss: 0.8147, Accuracy: 2982/3870 (77%)\n",
      "\n",
      "Train Epoch: 5 [0/35339 (0%)]\tLoss: 0.428582\n",
      "Train Epoch: 5 [5120/35339 (14%)]\tLoss: 0.414198\n",
      "Train Epoch: 5 [10240/35339 (29%)]\tLoss: 0.385831\n",
      "Train Epoch: 5 [15360/35339 (43%)]\tLoss: 0.386151\n",
      "Train Epoch: 5 [20480/35339 (57%)]\tLoss: 0.340759\n",
      "Train Epoch: 5 [25600/35339 (71%)]\tLoss: 0.300496\n",
      "Train Epoch: 5 [30720/35339 (86%)]\tLoss: 0.275402\n",
      "\n",
      "Validation set: Average loss: 0.7128, Accuracy: 3143/3870 (81%)\n",
      "\n",
      "\n",
      "Saved model to conv4_25_5.pth. You can run `python evaluate.py --model conv4_25_5.pth` to generate the Kaggle formatted csv file\n",
      "Train Epoch: 6 [0/35339 (0%)]\tLoss: 0.259117\n",
      "Train Epoch: 6 [5120/35339 (14%)]\tLoss: 0.255090\n",
      "Train Epoch: 6 [10240/35339 (29%)]\tLoss: 0.236318\n",
      "Train Epoch: 6 [15360/35339 (43%)]\tLoss: 0.272488\n",
      "Train Epoch: 6 [20480/35339 (57%)]\tLoss: 0.312053\n",
      "Train Epoch: 6 [25600/35339 (71%)]\tLoss: 0.213774\n",
      "Train Epoch: 6 [30720/35339 (86%)]\tLoss: 0.231909\n",
      "\n",
      "Validation set: Average loss: 0.6738, Accuracy: 3199/3870 (82%)\n",
      "\n",
      "Train Epoch: 7 [0/35339 (0%)]\tLoss: 0.198340\n",
      "Train Epoch: 7 [5120/35339 (14%)]\tLoss: 0.240445\n",
      "Train Epoch: 7 [10240/35339 (29%)]\tLoss: 0.196460\n",
      "Train Epoch: 7 [15360/35339 (43%)]\tLoss: 0.238332\n",
      "Train Epoch: 7 [20480/35339 (57%)]\tLoss: 0.235393\n",
      "Train Epoch: 7 [25600/35339 (71%)]\tLoss: 0.215624\n",
      "Train Epoch: 7 [30720/35339 (86%)]\tLoss: 0.170893\n",
      "\n",
      "Validation set: Average loss: 0.6295, Accuracy: 3243/3870 (83%)\n",
      "\n",
      "Train Epoch: 8 [0/35339 (0%)]\tLoss: 0.172431\n",
      "Train Epoch: 8 [5120/35339 (14%)]\tLoss: 0.258172\n",
      "Train Epoch: 8 [10240/35339 (29%)]\tLoss: 0.190280\n",
      "Train Epoch: 8 [15360/35339 (43%)]\tLoss: 0.313321\n",
      "Train Epoch: 8 [20480/35339 (57%)]\tLoss: 0.187025\n",
      "Train Epoch: 8 [25600/35339 (71%)]\tLoss: 0.203804\n",
      "Train Epoch: 8 [30720/35339 (86%)]\tLoss: 0.193651\n",
      "\n",
      "Validation set: Average loss: 0.6503, Accuracy: 3277/3870 (84%)\n",
      "\n",
      "Train Epoch: 9 [0/35339 (0%)]\tLoss: 0.187808\n",
      "Train Epoch: 9 [5120/35339 (14%)]\tLoss: 0.241172\n",
      "Train Epoch: 9 [10240/35339 (29%)]\tLoss: 0.214066\n",
      "Train Epoch: 9 [15360/35339 (43%)]\tLoss: 0.215200\n",
      "Train Epoch: 9 [20480/35339 (57%)]\tLoss: 0.126865\n",
      "Train Epoch: 9 [25600/35339 (71%)]\tLoss: 0.137945\n",
      "Train Epoch: 9 [30720/35339 (86%)]\tLoss: 0.184878\n",
      "\n",
      "Validation set: Average loss: 0.6878, Accuracy: 3206/3870 (82%)\n",
      "\n",
      "Train Epoch: 10 [0/35339 (0%)]\tLoss: 0.283737\n",
      "Train Epoch: 10 [5120/35339 (14%)]\tLoss: 0.253259\n",
      "Train Epoch: 10 [10240/35339 (29%)]\tLoss: 0.216335\n",
      "Train Epoch: 10 [15360/35339 (43%)]\tLoss: 0.165147\n",
      "Train Epoch: 10 [20480/35339 (57%)]\tLoss: 0.190554\n",
      "Train Epoch: 10 [25600/35339 (71%)]\tLoss: 0.112613\n",
      "Train Epoch: 10 [30720/35339 (86%)]\tLoss: 0.153870\n",
      "\n",
      "Validation set: Average loss: 0.6007, Accuracy: 3330/3870 (86%)\n",
      "\n",
      "\n",
      "Saved model to conv4_25_10.pth. You can run `python evaluate.py --model conv4_25_10.pth` to generate the Kaggle formatted csv file\n",
      "Train Epoch: 11 [0/35339 (0%)]\tLoss: 0.156371\n",
      "Train Epoch: 11 [5120/35339 (14%)]\tLoss: 0.169572\n",
      "Train Epoch: 11 [10240/35339 (29%)]\tLoss: 0.143028\n",
      "Train Epoch: 11 [15360/35339 (43%)]\tLoss: 0.146191\n",
      "Train Epoch: 11 [20480/35339 (57%)]\tLoss: 0.157038\n",
      "Train Epoch: 11 [25600/35339 (71%)]\tLoss: 0.152504\n",
      "Train Epoch: 11 [30720/35339 (86%)]\tLoss: 0.127701\n",
      "\n",
      "Validation set: Average loss: 0.6321, Accuracy: 3383/3870 (87%)\n",
      "\n",
      "Train Epoch: 12 [0/35339 (0%)]\tLoss: 0.151700\n",
      "Train Epoch: 12 [5120/35339 (14%)]\tLoss: 0.257810\n",
      "Train Epoch: 12 [10240/35339 (29%)]\tLoss: 0.191596\n",
      "Train Epoch: 12 [15360/35339 (43%)]\tLoss: 0.130788\n",
      "Train Epoch: 12 [20480/35339 (57%)]\tLoss: 0.114304\n",
      "Train Epoch: 12 [25600/35339 (71%)]\tLoss: 0.132948\n",
      "Train Epoch: 12 [30720/35339 (86%)]\tLoss: 0.181597\n",
      "\n",
      "Validation set: Average loss: 0.7454, Accuracy: 3292/3870 (85%)\n",
      "\n",
      "Train Epoch: 13 [0/35339 (0%)]\tLoss: 0.110234\n",
      "Train Epoch: 13 [5120/35339 (14%)]\tLoss: 0.131339\n",
      "Train Epoch: 13 [10240/35339 (29%)]\tLoss: 0.192304\n",
      "Train Epoch: 13 [15360/35339 (43%)]\tLoss: 0.091799\n",
      "Train Epoch: 13 [20480/35339 (57%)]\tLoss: 0.122361\n",
      "Train Epoch: 13 [25600/35339 (71%)]\tLoss: 0.137640\n",
      "Train Epoch: 13 [30720/35339 (86%)]\tLoss: 0.102455\n",
      "\n",
      "Validation set: Average loss: 0.6032, Accuracy: 3344/3870 (86%)\n",
      "\n",
      "Train Epoch: 14 [0/35339 (0%)]\tLoss: 0.109338\n",
      "Train Epoch: 14 [5120/35339 (14%)]\tLoss: 0.136149\n",
      "Train Epoch: 14 [10240/35339 (29%)]\tLoss: 0.110016\n",
      "Train Epoch: 14 [15360/35339 (43%)]\tLoss: 0.140009\n",
      "Train Epoch: 14 [20480/35339 (57%)]\tLoss: 0.129206\n",
      "Train Epoch: 14 [25600/35339 (71%)]\tLoss: 0.120509\n",
      "Train Epoch: 14 [30720/35339 (86%)]\tLoss: 0.104340\n",
      "\n",
      "Validation set: Average loss: 0.5978, Accuracy: 3353/3870 (86%)\n",
      "\n",
      "Train Epoch: 15 [0/35339 (0%)]\tLoss: 0.175260\n",
      "Train Epoch: 15 [5120/35339 (14%)]\tLoss: 0.118488\n",
      "Train Epoch: 15 [10240/35339 (29%)]\tLoss: 0.110290\n",
      "Train Epoch: 15 [15360/35339 (43%)]\tLoss: 0.086928\n",
      "Train Epoch: 15 [20480/35339 (57%)]\tLoss: 0.109018\n",
      "Train Epoch: 15 [25600/35339 (71%)]\tLoss: 0.098599\n",
      "Train Epoch: 15 [30720/35339 (86%)]\tLoss: 0.097115\n",
      "\n",
      "Validation set: Average loss: 0.6645, Accuracy: 3387/3870 (87%)\n",
      "\n",
      "\n",
      "Saved model to conv4_25_15.pth. You can run `python evaluate.py --model conv4_25_15.pth` to generate the Kaggle formatted csv file\n",
      "Train Epoch: 16 [0/35339 (0%)]\tLoss: 0.077410\n",
      "Train Epoch: 16 [5120/35339 (14%)]\tLoss: 0.088145\n",
      "Train Epoch: 16 [10240/35339 (29%)]\tLoss: 0.104726\n",
      "Train Epoch: 16 [15360/35339 (43%)]\tLoss: 0.105866\n",
      "Train Epoch: 16 [20480/35339 (57%)]\tLoss: 0.074870\n",
      "Train Epoch: 16 [25600/35339 (71%)]\tLoss: 0.103874\n",
      "Train Epoch: 16 [30720/35339 (86%)]\tLoss: 0.083508\n",
      "\n",
      "Validation set: Average loss: 0.6885, Accuracy: 3403/3870 (87%)\n",
      "\n",
      "Train Epoch: 17 [0/35339 (0%)]\tLoss: 0.103986\n",
      "Train Epoch: 17 [5120/35339 (14%)]\tLoss: 0.086227\n",
      "Train Epoch: 17 [10240/35339 (29%)]\tLoss: 0.128751\n",
      "Train Epoch: 17 [15360/35339 (43%)]\tLoss: 0.083048\n",
      "Train Epoch: 17 [20480/35339 (57%)]\tLoss: 0.105930\n",
      "Train Epoch: 17 [25600/35339 (71%)]\tLoss: 0.081999\n",
      "Train Epoch: 17 [30720/35339 (86%)]\tLoss: 0.063908\n",
      "\n",
      "Validation set: Average loss: 0.6636, Accuracy: 3380/3870 (87%)\n",
      "\n",
      "Train Epoch: 18 [0/35339 (0%)]\tLoss: 0.078341\n",
      "Train Epoch: 18 [5120/35339 (14%)]\tLoss: 0.091338\n",
      "Train Epoch: 18 [10240/35339 (29%)]\tLoss: 0.093834\n",
      "Train Epoch: 18 [15360/35339 (43%)]\tLoss: 0.083935\n",
      "Train Epoch: 18 [20480/35339 (57%)]\tLoss: 0.051694\n",
      "Train Epoch: 18 [25600/35339 (71%)]\tLoss: 0.085845\n",
      "Train Epoch: 18 [30720/35339 (86%)]\tLoss: 0.106491\n",
      "\n",
      "Validation set: Average loss: 0.6704, Accuracy: 3372/3870 (87%)\n",
      "\n",
      "Train Epoch: 19 [0/35339 (0%)]\tLoss: 0.070257\n",
      "Train Epoch: 19 [5120/35339 (14%)]\tLoss: 0.060579\n",
      "Train Epoch: 19 [10240/35339 (29%)]\tLoss: 0.080569\n",
      "Train Epoch: 19 [15360/35339 (43%)]\tLoss: 0.049018\n",
      "Train Epoch: 19 [20480/35339 (57%)]\tLoss: 0.101822\n",
      "Train Epoch: 19 [25600/35339 (71%)]\tLoss: 0.080326\n",
      "Train Epoch: 19 [30720/35339 (86%)]\tLoss: 0.080773\n",
      "\n",
      "Validation set: Average loss: 0.6130, Accuracy: 3401/3870 (87%)\n",
      "\n",
      "Train Epoch: 20 [0/35339 (0%)]\tLoss: 0.094052\n",
      "Train Epoch: 20 [5120/35339 (14%)]\tLoss: 0.152289\n",
      "Train Epoch: 20 [10240/35339 (29%)]\tLoss: 0.069574\n",
      "Train Epoch: 20 [15360/35339 (43%)]\tLoss: 0.088794\n",
      "Train Epoch: 20 [20480/35339 (57%)]\tLoss: 0.089952\n",
      "Train Epoch: 20 [25600/35339 (71%)]\tLoss: 0.094050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 20 [30720/35339 (86%)]\tLoss: 0.072787\n",
      "\n",
      "Validation set: Average loss: 0.5770, Accuracy: 3441/3870 (88%)\n",
      "\n",
      "\n",
      "Saved model to conv4_25_20.pth. You can run `python evaluate.py --model conv4_25_20.pth` to generate the Kaggle formatted csv file\n",
      "Train Epoch: 21 [0/35339 (0%)]\tLoss: 0.070144\n",
      "Train Epoch: 21 [5120/35339 (14%)]\tLoss: 0.109877\n",
      "Train Epoch: 21 [10240/35339 (29%)]\tLoss: 0.086206\n",
      "Train Epoch: 21 [15360/35339 (43%)]\tLoss: 0.072418\n",
      "Train Epoch: 21 [20480/35339 (57%)]\tLoss: 0.080269\n",
      "Train Epoch: 21 [25600/35339 (71%)]\tLoss: 0.139223\n",
      "Train Epoch: 21 [30720/35339 (86%)]\tLoss: 0.112620\n",
      "\n",
      "Validation set: Average loss: 0.5555, Accuracy: 3407/3870 (88%)\n",
      "\n",
      "Train Epoch: 22 [0/35339 (0%)]\tLoss: 0.100357\n",
      "Train Epoch: 22 [5120/35339 (14%)]\tLoss: 0.071013\n",
      "Train Epoch: 22 [10240/35339 (29%)]\tLoss: 0.073010\n",
      "Train Epoch: 22 [15360/35339 (43%)]\tLoss: 0.038885\n",
      "Train Epoch: 22 [20480/35339 (57%)]\tLoss: 0.063011\n",
      "Train Epoch: 22 [25600/35339 (71%)]\tLoss: 0.065824\n",
      "Train Epoch: 22 [30720/35339 (86%)]\tLoss: 0.045072\n",
      "\n",
      "Validation set: Average loss: 0.6440, Accuracy: 3379/3870 (87%)\n",
      "\n",
      "Train Epoch: 23 [0/35339 (0%)]\tLoss: 0.051367\n",
      "Train Epoch: 23 [5120/35339 (14%)]\tLoss: 0.050766\n",
      "Train Epoch: 23 [10240/35339 (29%)]\tLoss: 0.096325\n",
      "Train Epoch: 23 [15360/35339 (43%)]\tLoss: 0.104458\n",
      "Train Epoch: 23 [20480/35339 (57%)]\tLoss: 0.091134\n",
      "Train Epoch: 23 [25600/35339 (71%)]\tLoss: 0.043419\n",
      "Train Epoch: 23 [30720/35339 (86%)]\tLoss: 0.085003\n",
      "\n",
      "Validation set: Average loss: 0.6678, Accuracy: 3352/3870 (86%)\n",
      "\n",
      "Train Epoch: 24 [0/35339 (0%)]\tLoss: 0.064369\n",
      "Train Epoch: 24 [5120/35339 (14%)]\tLoss: 0.105697\n",
      "Train Epoch: 24 [10240/35339 (29%)]\tLoss: 0.076735\n",
      "Train Epoch: 24 [15360/35339 (43%)]\tLoss: 0.068318\n",
      "Train Epoch: 24 [20480/35339 (57%)]\tLoss: 0.056459\n",
      "Train Epoch: 24 [25600/35339 (71%)]\tLoss: 0.039674\n",
      "Train Epoch: 24 [30720/35339 (86%)]\tLoss: 0.055666\n",
      "\n",
      "Validation set: Average loss: 0.7366, Accuracy: 3376/3870 (87%)\n",
      "\n",
      "Train Epoch: 25 [0/35339 (0%)]\tLoss: 0.060030\n",
      "Train Epoch: 25 [5120/35339 (14%)]\tLoss: 0.069954\n",
      "Train Epoch: 25 [10240/35339 (29%)]\tLoss: 0.073806\n",
      "Train Epoch: 25 [15360/35339 (43%)]\tLoss: 0.059398\n",
      "Train Epoch: 25 [20480/35339 (57%)]\tLoss: 0.063411\n",
      "Train Epoch: 25 [25600/35339 (71%)]\tLoss: 0.052784\n",
      "Train Epoch: 25 [30720/35339 (86%)]\tLoss: 0.044426\n",
      "\n",
      "Validation set: Average loss: 0.6769, Accuracy: 3390/3870 (87%)\n",
      "\n",
      "\n",
      "Saved model to conv4_25_25.pth. You can run `python evaluate.py --model conv4_25_25.pth` to generate the Kaggle formatted csv file\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    validation()\n",
    "    if epoch%5 == 0:\n",
    "        model_file = exp_name + str(epoch) + '.pth'\n",
    "        torch.save(model.state_dict(), model_file)\n",
    "        print('\\nSaved model to ' + model_file + '. You can run `python evaluate.py --model ' + model_file + '` to generate the Kaggle formatted csv file')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv4Net(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=10816, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=43, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('conv4_25_25.pth')\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12630 [00:00<?, ?it/s]/home/rm5310/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/rm5310/pyenv/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      " 28%|██▊       | 3497/12630 [00:50<02:10, 69.89it/s]"
     ]
    }
   ],
   "source": [
    "test_dir = 'images/test_images'\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')\n",
    "\n",
    "outfile = exp_name+'out.csv'\n",
    "        \n",
    "output_file = open(outfile, \"w\")\n",
    "output_file.write(\"Filename,ClassId\\n\")\n",
    "for f in tqdm(os.listdir(test_dir)):\n",
    "    if 'ppm' in f:\n",
    "        data = test_transforms(pil_loader(test_dir + '/' + f))\n",
    "        data = data.view(1, data.size(0), data.size(1), data.size(2))\n",
    "        data = Variable(data, volatile=True).to(device)\n",
    "        output = model(data)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "\n",
    "        file_id = f[0:5]\n",
    "        output_file.write(\"%s,%d\\n\" % (file_id, pred))\n",
    "\n",
    "output_file.close()\n",
    "\n",
    "print(\"Succesfully wrote \" + outfile + ', you can upload this file to the kaggle '\n",
    "      'competition at https://www.kaggle.com/c/nyu-cv-fall-2018/')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
